{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be622de8",
   "metadata": {},
   "source": [
    "# Water Potability Prediction - Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55822153",
   "metadata": {},
   "source": [
    "### - Water potability refers to the safety water for human consumption\n",
    "### - Potable water is free from harmful contaminants and bacteria and is safe for drinking and food preparation\n",
    "### - There are various method to ensure water potability, including filtration, and treatment processes such as UV filtration and reverse osmosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca9f13",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcb379",
   "metadata": {},
   "source": [
    "## 1. Library Improting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d87193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42767297",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83c845",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6845d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data = pd.read_csv('water_potability.csv')\n",
    "water_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45c2b9",
   "metadata": {},
   "source": [
    "#### - ph : pH of water \n",
    "#### - Hardness : Capacity of water to precipitate soap in mg/L\n",
    "#### - Solids : Total dissolved solids in ppm\n",
    "#### - Chloramines : Amounts of Chloramines in ppm\n",
    "#### - Sulfate : Amounts of Sulfates dissolved in mg/L\n",
    "#### - Conductivity : Electrical conductivity of water in uS/cm\n",
    "#### - Organic_carbon : Amount of organic carbon in ppm\n",
    "#### - Trihalomethanes : Amount of Trihalomethanes in mg/L\n",
    "#### - Turbidity : Measure of light emiting property of water in NTV ( Nephelometric Turbidity Units )\n",
    "#### - Potability : Indicates if water is safe for human consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f94a0",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = water_data.isnull().sum().reset_index()\n",
    "null_df.columns = ['Columns', 'Null_count']\n",
    "null_df['%miss_value'] = round(null_df['Null_count']/len(water_data), 2)*100\n",
    "null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5711dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(water_data.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da1982",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa9a2b",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data['ph'].plot(kind = 'hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data['Sulfate'].plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a87369",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data['Trihalomethanes'].plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "water_data['Trihalomethanes'].plot(kind='kde', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6bd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data['ph'] = water_data['ph'].fillna(water_data['ph'].mean())\n",
    "water_data['Sulfate'] = water_data['Sulfate'].fillna(water_data['Sulfate'].mean())\n",
    "water_data['Trihalomethanes'] = water_data['Trihalomethanes'].fillna(water_data['Trihalomethanes'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc63eba",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b3a93",
   "metadata": {},
   "source": [
    "## 4. Check for Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ca4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = water_data.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ca67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 16))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637deef8",
   "metadata": {},
   "source": [
    "dapat dilihat bahwa tidak ada korelasi yang signifikan dari variabel-variabel tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7821b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix1 = corr_matrix.abs()\n",
    "upper_tri = corr_matrix1.where(np.triu(np.ones(corr_matrix1.shape), k=1).astype(np.bool_))\n",
    "upper_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.triu(corr_matrix)\n",
    "sns.heatmap(water_data.corr(), annot=True, linewidth=.8, mask=matrix, cmap='rocket', cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca0f9a",
   "metadata": {},
   "source": [
    "ini adalah salah satu bentuk matrix korelasi yang lainnya, dapat menggunakan yang sebelumnya atau yang ini, sama saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hist_plot = water_data.hist(figsize=(20, 20), color = '#5F9EA0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water_data.columns :\n",
    "    sns.histplot(data=water_data, x=col, kde=True, hue='Potability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d77bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.groupby('Potability').mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water_data.columns :\n",
    "    sns.boxplot(data=water_data, x=col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(water_data['Potability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da749e8f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3966fb",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = water_data.drop(['Potability'], axis=1)\n",
    "y = water_data['Potability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0738d15",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9d5dc",
   "metadata": {},
   "source": [
    "## 6. Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83911df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8959b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = std_scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa268b",
   "metadata": {},
   "source": [
    "Mentransformasi data X dengan 'Zero Mean Value' dan satu standard deviasi. Tranformasi dimaksudkan untuk merubah dataset menjadi array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65286651",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d5247",
   "metadata": {},
   "source": [
    "## 7. Classification Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f1a70",
   "metadata": {},
   "source": [
    "### Model Development : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cf318",
   "metadata": {},
   "source": [
    "List of Models :\n",
    "1. LogisticRegressin\n",
    "2. DecisionTreeClassifier\n",
    "3. RandomForestClassifier\n",
    "4. ExtraTreesClasifier\n",
    "5. Support Vector Classifier\n",
    "6. KNeighborsClassifier\n",
    "7. GradientBoostingClassifier\n",
    "8. Naive-Bayer\n",
    "9. AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd92bd6",
   "metadata": {},
   "source": [
    "#### Importing Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56857de",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "DT = DecisionTreeClassifier()\n",
    "RF = RandomForestClassifier()\n",
    "ETC = ExtraTreesClassifier()\n",
    "SVM = SVC()\n",
    "KNN = KNeighborsClassifier()\n",
    "GBC = GradientBoostingClassifier()\n",
    "NB = GaussianNB()\n",
    "ABC = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LR, DT, RF, ETC, SVM, KNN, GBC, NB, ABC]\n",
    "features = X_scaled\n",
    "labels = y\n",
    "CV = 5\n",
    "accu_list = [] # Accuracy List\n",
    "ModelName = [] # Model Name List\n",
    "\n",
    "for model in models :\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv = CV)\n",
    "    accu_list.append(accuracies.mean()*100)\n",
    "    ModelName.append(model_name)\n",
    "\n",
    "model_acc_df = pd.DataFrame({\"Model\" : ModelName, \"Cross_Val_Accuracy\" : accu_list})\n",
    "model_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900653a7",
   "metadata": {},
   "source": [
    "ini merupakan presentase akurasi setiap model. Dapat dilihat bahwa yang terbesar adalah SVM, ETC, dan RF. Maka, ketiga model ini akan dicek lebih lanjut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM.fit(X_train, y_train)\n",
    "ETC.fit(X_train, y_train)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = RF.predict(X_test)\n",
    "y_pred_svm = SVM.predict(X_test)\n",
    "y_pred_etc = ETC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d878e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_etc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207d002",
   "metadata": {},
   "source": [
    "karena RandomForestClassifier merupakan model yang paling lengkap dan mudah untuk dijelaskan, maka ada digunakan model RF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816710d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "y_scores = ETC.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "y_scores = RF.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976f5f7",
   "metadata": {},
   "source": [
    "#### Best Model - Random Forest : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "params_RF = {\"min_samples_split\" : [2, 6],\n",
    "            \"min_samples_leaf\" : [1, 4],\n",
    "            \"n_estimators\" : [100, 200, 300],\n",
    "            \"criterion\" : [\"gini\", 'entropy']\n",
    "            }\n",
    "\n",
    "cv_method = StratifiedKFold(n_splits=3)\n",
    "GridSearchCV_RF = GridSearchCV(estimator  = ExtraTreesClassifier(),\n",
    "                              param_grid = params_RF,\n",
    "                              cv = cv_method,\n",
    "                              verbose = 1,\n",
    "                              n_jobs = 2,\n",
    "                              scoring = \"accuracy\",\n",
    "                              return_train_score = True\n",
    "                              )\n",
    "\n",
    "GridSearchCV_RF.fit(X_train, y_train)\n",
    "best_params_RF = GridSearchCV_RF.best_params_\n",
    "print(\"Best Hyperparameters for Random Forest are = \", best_params_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a202b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = GridSearchCV_RF.best_estimator_\n",
    "best_estimator.fit(X_train, y_train)\n",
    "y_pred_best = best_estimator.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53970ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy of Random Forest Model = {round(accuracy_score(y_test, y_pred_best)*100, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12981a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sklearn.metrics\n",
    "\n",
    "Y_pred = model.predict(y_pred_rf)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_pred_rf.classes, y_pred))\n",
    "confusion_array = sklearn.metrics.confusion_matrix(y_pred_rf.classes, y_pred)\n",
    "\n",
    "print('True Negative = ', confusion_array[0,0])\n",
    "print('False Negative = ', confusion_array[1,0])\n",
    "print('True Positive = ', confusion_array[1,1])\n",
    "print('False Positive = ', confusion_array[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec295e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e347f05",
   "metadata": {},
   "source": [
    "## 8. Fuzzy Inference System Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff307d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # remove WARNING Messages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# parameter class fis parameters\n",
    "\n",
    "\n",
    "class fis_parameters():\n",
    "    def __init__(self, n_input: int = 3, n_memb: int = 3, batch_size: int = 16, n_epochs: int = 25, memb_func: str = 'gaussian', optimizer: str = 'sgd', loss: str = 'mse'):\n",
    "        self.n_input = n_input  # no. of Regressors\n",
    "        self.n_memb = n_memb  # no. of fuzzy memberships\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.memb_func = memb_func  # 'gaussian' / 'gbellmf'\n",
    "        self.optimizer = optimizer   # sgd / adam /\n",
    "        self.loss = loss  # mse / mae\n",
    "\n",
    "\n",
    "# Main Class ANFIS\n",
    "class ANFIS:\n",
    "    def __init__(self, n_input: int, n_memb: int, batch_size: int = 16, memb_func: str = 'gaussian', name: str = 'MyAnfis'):\n",
    "        self.n = n_input\n",
    "        self.m = n_memb\n",
    "        self.batch_size = batch_size\n",
    "        self.memb_func = memb_func\n",
    "        input_ = keras.layers.Input(\n",
    "            shape=(n_input), name='inputLayer', batch_size=self.batch_size)\n",
    "        L1 = FuzzyLayer(n_input, n_memb, memb_func, name='fuzzyLayer')(input_)\n",
    "        L2 = RuleLayer(n_input, n_memb, name='ruleLayer')(L1)\n",
    "        L3 = NormLayer(name='normLayer')(L2)\n",
    "        L4 = DefuzzLayer(n_input, n_memb, name='defuzzLayer')(L3, input_)\n",
    "        L5 = SummationLayer(name='sumLayer')(L4)\n",
    "        self.model = keras.Model(inputs=[input_], outputs=[L5], name=name)\n",
    "        self.update_weights()\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.model.predict(X, batch_size=self.batch_size)\n",
    "\n",
    "    def update_weights(self):\n",
    "        # premise parameters (mu&sigma for gaussian // a/b/c for bell-shaped)\n",
    "        if self.memb_func == 'gaussian':\n",
    "            self.mus, self.sigmas = self.model.get_layer(\n",
    "                'fuzzyLayer').get_weights()\n",
    "        elif self.memb_func == 'gbellmf':\n",
    "            self.a, self.b, self.c = self.model.get_layer(\n",
    "                'fuzzyLayer').get_weights()\n",
    "        # consequence parameters\n",
    "        self.bias, self.weights = self.model.get_layer(\n",
    "            'defuzzLayer').get_weights()\n",
    "\n",
    "    def plotmfs(self, show_initial_weights=False):\n",
    "        n_input = self.n\n",
    "        n_memb = self.m\n",
    "\n",
    "        if self.memb_func == 'gaussian':\n",
    "            mus, sigmas = np.around(self.model.get_layer(\n",
    "                'fuzzyLayer').get_weights(), 2)\n",
    "            mus, sigmas = mus.reshape(\n",
    "                (n_memb, n_input, 1)), sigmas.reshape(n_memb, n_input, 1)\n",
    "\n",
    "            xn = np.linspace(np.min(mus) - 2 * np.max(abs(sigmas)),\n",
    "                             np.max(mus) + 2 * np.max(abs(sigmas)), 100).reshape((1, 1, -1))\n",
    "            xn = np.tile(xn, (n_memb, n_input, 1))\n",
    "\n",
    "            # broadcast all curves in one array\n",
    "            memb_curves = np.exp(-np.square((xn - mus)) / np.square(sigmas))\n",
    "\n",
    "            if show_initial_weights:\n",
    "                mus_init, sigmas_init = np.around(self.init_weights, 2)\n",
    "                mus_init, sigmas_init = mus_init.reshape(\n",
    "                    n_memb, n_input, 1), sigmas_init.reshape(n_memb, n_input, 1)\n",
    "                init_curves = np.exp(-np.square((xn - mus_init)\n",
    "                                                ) / np.square(sigmas_init))\n",
    "\n",
    "        elif self.memb_func == 'gbellmf':\n",
    "            a, b, c = np.around(self.model.get_layer(\n",
    "                'fuzzyLayer').get_weights(), 2)\n",
    "            a, b, c = a.reshape((n_memb, n_input, 1)), b.reshape(\n",
    "                n_memb, n_input, 1), c.reshape(n_memb, n_input, 1)\n",
    "\n",
    "            xn = np.linspace(np.min(c) - 2 * np.max(abs(a)),\n",
    "                             np.max(c) + 2 * np.max(abs(a)), 100).reshape((1, 1, -1))\n",
    "            xn = np.tile(xn, (n_memb, n_input, 1))\n",
    "\n",
    "            # broadcast all curves in one array\n",
    "            memb_curves = 1 / (1 + np.square((xn - c) / a)**b)\n",
    "\n",
    "            if show_initial_weights:\n",
    "                a_init, b_init, c_init = np.around(self.init_weights, 2)\n",
    "                a_init, b_init, c_init = a_init.reshape((n_memb, n_input, 1)), b_init.reshape(\n",
    "                    n_memb, n_input, 1), c_init.reshape(n_memb, n_input, 1)\n",
    "                init_curves = 1 / \\\n",
    "                    (1 + np.square((xn - c_init) / a_init)**b_init)\n",
    "\n",
    "        elif self.memb_func == 'sigmoid':\n",
    "            gammas, c = np.around(self.model.get_layer(\n",
    "                'fuzzyLayer').get_weights(), 2)\n",
    "            gammas, c = gammas.reshape(\n",
    "                (n_memb, n_input, 1)), c.reshape(n_memb, n_input, 1)\n",
    "\n",
    "            xn = np.linspace(np.min(c) - 2 * np.max(abs(c)), np.max(c) + 2 * np.max(\n",
    "                abs(c)), 100).reshape((1, 1, -1))  # TODO: change confidence bands\n",
    "            xn = np.tile(xn, (n_memb, n_input, 1))\n",
    "\n",
    "            # broadcast all curves in one array\n",
    "            memb_curves = 1 / (1 + np.exp(-gammas * (xn - c)))\n",
    "\n",
    "            if show_initial_weights:\n",
    "                gammas_init, c_init = np.around(self.init_weights, 2)\n",
    "                gammas_init, c_init = gammas_init.reshape(\n",
    "                    n_memb, n_input, 1), c_init.reshape(n_memb, n_input, 1)\n",
    "                init_curves = 1 / (1 + np.exp(-gammas_init * (xn - c_init)))\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=n_input, ncols=1, figsize=(8, self.n * 3))\n",
    "        fig.suptitle('Membership functions', size=16)\n",
    "        for n in range(self.n):\n",
    "            axs[n].grid(True)\n",
    "            axs[n].set_title(f'Input {n+1}')\n",
    "            for m in range(self.m):\n",
    "                axs[n].plot(xn[m, n, :], memb_curves[m, n, :])\n",
    "\n",
    "        if show_initial_weights:  # plot initial membership curve\n",
    "            for n in range(self.n):\n",
    "                axs[n].set_prop_cycle(None)  # reset color cycle\n",
    "                for m in range(self.m):\n",
    "                    axs[n].plot(xn[m, n, :], init_curves[m, n, :],\n",
    "                                '--', alpha=.5)\n",
    "        plt.show()\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        # save initial weights in the anfis class\n",
    "        self.init_weights = self.model.get_layer('fuzzyLayer').get_weights()\n",
    "\n",
    "        # fit model & update weights in the anfis class\n",
    "        history = self.model.fit(X, y, **kwargs)\n",
    "        self.update_weights()\n",
    "\n",
    "        # clear the graphs\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        return history\n",
    "\n",
    "    def get_memberships(self, Xs):\n",
    "        intermediate_layer_model = keras.Model(inputs=self.model.input,\n",
    "                                               outputs=self.model.get_layer('normLayer').output)\n",
    "\n",
    "        intermediate_L2_output = intermediate_layer_model.predict(Xs)\n",
    "\n",
    "        return intermediate_L2_output\n",
    "\n",
    "\n",
    "# Custom weight initializer\n",
    "def equally_spaced_initializer(shape, minval=-1.5, maxval=1.5, dtype=tf.float32):\n",
    "    \"\"\"\n",
    "    Custom weight initializer:\n",
    "        euqlly spaced weights along an operating range of [minval, maxval].\n",
    "    \"\"\"\n",
    "    linspace = tf.reshape(tf.linspace(minval, maxval, shape[0]),\n",
    "                          (-1, 1))\n",
    "    return tf.Variable(tf.tile(linspace, (1, shape[1])))\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "class FuzzyLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_input, n_memb, memb_func='gaussian', **kwargs):\n",
    "        super(FuzzyLayer, self).__init__(**kwargs)\n",
    "        self.n = n_input\n",
    "        self.m = n_memb\n",
    "        self.memb_func = memb_func\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.batch_size = batch_input_shape[0]\n",
    "\n",
    "        if self.memb_func == 'gbellmf':\n",
    "            self.a = self.add_weight(name='a',\n",
    "                                     shape=(self.m, self.n),\n",
    "                                     initializer=keras.initializers.RandomUniform(\n",
    "                                         minval=.7, maxval=1.3, seed=1),\n",
    "                                     #initializer = 'ones',\n",
    "                                     trainable=True)\n",
    "            self.b = self.add_weight(name='b',\n",
    "                                     shape=(self.m, self.n),\n",
    "                                     initializer=keras.initializers.RandomUniform(\n",
    "                                         minval=.7, maxval=1.3, seed=1),\n",
    "                                     #initializer = 'ones',\n",
    "                                     trainable=True)\n",
    "            self.c = self.add_weight(name='c',\n",
    "                                     shape=(self.m, self.n),\n",
    "                                     initializer=equally_spaced_initializer,\n",
    "                                     #initializer = keras.initializers.RandomUniform(minval=-1.5, maxval=1.5, seed=1),\n",
    "                                     #initializer = 'zeros',\n",
    "                                     trainable=True)\n",
    "\n",
    "        elif self.memb_func == 'gaussian':\n",
    "            self.mu = self.add_weight(name='mu',\n",
    "                                      shape=(self.m, self.n),\n",
    "                                      initializer=equally_spaced_initializer,\n",
    "                                      #initializer = keras.initializers.RandomUniform(minval=-1.5, maxval=1.5, seed=1),\n",
    "                                      #initializer = 'zeros',\n",
    "                                      trainable=True)\n",
    "            self.sigma = self.add_weight(name='sigma',\n",
    "                                         shape=(self.m, self.n),\n",
    "                                         initializer=keras.initializers.RandomUniform(\n",
    "                                             minval=.7, maxval=1.3, seed=1),\n",
    "                                         #initializer = 'ones',\n",
    "                                         trainable=True)\n",
    "\n",
    "        elif self.memb_func == 'sigmoid':\n",
    "            self.gamma = self.add_weight(name='gamma',\n",
    "                                         shape=(self.m, self.n),\n",
    "                                         initializer=equally_spaced_initializer,  # 'ones',\n",
    "                                         trainable=True)\n",
    "\n",
    "            self.c = self.add_weight(name='c',\n",
    "                                     shape=(self.m, self.n),\n",
    "                                     initializer=equally_spaced_initializer,  # 'ones',\n",
    "                                     trainable=True)\n",
    "\n",
    "        # Be sure to call this at the end\n",
    "        super(FuzzyLayer, self).build(batch_input_shape)\n",
    "\n",
    "    def call(self, x_inputs):\n",
    "        if self.memb_func == 'gbellmf':\n",
    "            L1_output = 1 / (1 +\n",
    "                             tf.math.pow(\n",
    "                                 tf.square(tf.subtract(\n",
    "                                     tf.reshape(\n",
    "                                         tf.tile(x_inputs, (1, self.m)), (-1, self.m, self.n)), self.c\n",
    "                                 ) / self.a), self.b)\n",
    "                             )\n",
    "        elif self.memb_func == 'gaussian':\n",
    "            L1_output = tf.exp(-1 *\n",
    "                               tf.square(tf.subtract(\n",
    "                                   tf.reshape(\n",
    "                                       tf.tile(x_inputs, (1, self.m)), (-1, self.m, self.n)), self.mu\n",
    "                               )) / tf.square(self.sigma))\n",
    "\n",
    "        elif self.memb_func == 'sigmoid':\n",
    "            L1_output = tf.math.divide(1,\n",
    "                                       tf.math.exp(-self.gamma *\n",
    "                                                   tf.subtract(\n",
    "                                                       tf.reshape(\n",
    "                                                           tf.tile(x_inputs, (1, self.m)), (-1, self.m, self.n)), self.c)\n",
    "                                                   )\n",
    "                                       )\n",
    "        return L1_output\n",
    "\n",
    "\n",
    "# Layer 2\n",
    "class RuleLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_input, n_memb, **kwargs):\n",
    "        super(RuleLayer, self).__init__(**kwargs)\n",
    "        self.n = n_input\n",
    "        self.m = n_memb\n",
    "        self.batch_size = None\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.batch_size = batch_input_shape[0]\n",
    "        # self.batch_size = tf.shape(batch_input_shape)[0]\n",
    "        # Be sure to call this at the end\n",
    "        super(RuleLayer, self).build(batch_input_shape)\n",
    "\n",
    "    def call(self, input_):\n",
    "        if self.n == 2:\n",
    "            L2_output = tf.reshape(input_[:, :, 0], [self.batch_size, -1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 1], [self.batch_size, 1, -1])\n",
    "        elif self.n == 3:\n",
    "            L2_output = tf.reshape(input_[:, :, 0], [self.batch_size, -1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 1], [self.batch_size, 1, -1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 2], [self.batch_size, 1, 1, -1])\n",
    "        elif self.n == 4:\n",
    "            L2_output = tf.reshape(input_[:, :, 0], [self.batch_size, -1, 1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 1], [self.batch_size, 1, -1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 2], [self.batch_size, 1, 1, -1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 3], [self.batch_size, 1, 1, 1, -1])\n",
    "        elif self.n == 5:\n",
    "            L2_output = tf.reshape(input_[:, :, 0], [self.batch_size, -1, 1, 1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 1], [self.batch_size, 1, -1, 1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 2], [self.batch_size, 1, 1, -1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 3], [self.batch_size, 1, 1, 1, -1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 4], [self.batch_size, 1, 1, 1, 1, -1])\n",
    "        elif self.n == 6:\n",
    "            L2_output = tf.reshape(input_[:, :, 0], [self.batch_size, -1, 1, 1, 1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 1], [self.batch_size, 1, -1, 1, 1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 2], [self.batch_size, 1, 1, -1, 1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 3], [self.batch_size, 1, 1, 1, -1, 1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 4], [self.batch_size, 1, 1, 1, 1, -1, 1]) * \\\n",
    "                tf.reshape(input_[:, :, 5], [\n",
    "                           self.batch_size, 1, 1, 1, 1, 1, -1])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'This ANFIS implementation works with 2 to 6 inputs.')\n",
    "\n",
    "        return tf.reshape(L2_output, [self.batch_size, -1])\n",
    "\n",
    "\n",
    "# Layer 3\n",
    "class NormLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, w):\n",
    "        w_sum = tf.reshape(tf.reduce_sum(w, axis=1), (-1, 1))\n",
    "        w_norm = w / w_sum\n",
    "        return w_norm\n",
    "\n",
    "\n",
    "# Layer 4\n",
    "class DefuzzLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_input, n_memb, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n = n_input\n",
    "        self.m = n_memb\n",
    "\n",
    "        self.CP_bias = self.add_weight(name='Consequence_bias',\n",
    "                                       shape=(1, self.m ** self.n),\n",
    "                                       initializer=keras.initializers.RandomUniform(\n",
    "                                           minval=-2, maxval=2),\n",
    "                                       # initializer = 'ones',\n",
    "                                       trainable=True)\n",
    "        self.CP_weight = self.add_weight(name='Consequence_weight',\n",
    "                                         shape=(self.n, self.m ** self.n),\n",
    "                                         initializer=keras.initializers.RandomUniform(\n",
    "                                             minval=-2, maxval=2),\n",
    "                                         # initializer = 'ones',\n",
    "                                         trainable=True)\n",
    "\n",
    "    def call(self, w_norm, input_):\n",
    "\n",
    "        L4_L2_output = tf.multiply(w_norm,\n",
    "                                   tf.matmul(input_, self.CP_weight) + self.CP_bias)\n",
    "        return L4_L2_output  # Defuzzyfied Layer\n",
    "\n",
    "\n",
    "# Layer 5\n",
    "class SummationLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.batch_size = batch_input_shape[0]\n",
    "        #self.batch_size = tf.shape(batch_input_shape)[0]\n",
    "        # Be sure to call this at the end\n",
    "        super(SummationLayer, self).build(batch_input_shape)\n",
    "\n",
    "    def call(self, input_):\n",
    "        L5_L2_output = tf.reduce_sum(input_, axis=1)\n",
    "        L5_L2_output = tf.reshape(L5_L2_output, (-1, 1))\n",
    "        return L5_L2_output\n",
    "\n",
    "    # def compute_L2_output_shape(self, batch_input_shape):\n",
    "        # return tf.TensorShape([self.batch_size, 1])\n",
    "\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180bfda",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe936db",
   "metadata": {},
   "source": [
    "## 9. Classification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = water_data.iloc[2:3, 0:9].values.flatten().tolist()\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf64cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = float(input('Enter the pH Value = '))\n",
    "Hardness = float(input('Enter the Hardness Value = '))\n",
    "Solids = float(input('Enter the Solids Value = '))\n",
    "Chloramines = float(input('Enter the Chloramines Value  = '))\n",
    "Sulfate = float(input('Enter the Sulfate Value = '))\n",
    "Conductivity = float(input('Enter the Conductivity Value = '))\n",
    "Organic_carbon = float(input('Enter the Organic_Carbon Value = '))\n",
    "Trihalomethanes = float(input('Enter the Trihalomethanes Value = '))\n",
    "Turbidity = float(input('Enter the Turbidity Value = '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ffc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [ph, Hardness, Solids, Chloramines, Sulfate, Conductivity,\n",
    "             Organic_carbon, Trihalomethanes, Turbidity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab49724",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_data_input = std_scaler.transform([[ph, Hardness, Solids, Chloramines, Sulfate, Conductivity,\n",
    "                                         Organic_carbon, Trihalomethanes, Turbidity]])\n",
    "water_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = best_estimator.predict(water_data_input)\n",
    "model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_prediction[0] == 0:\n",
    "    print(\"Water is Not SAFE for Plant\")\n",
    "else :\n",
    "    print(\"Water is SAFE for Plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ad6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def water_Quality_Prediction(input_data) :\n",
    "    scaled_data = std_scaler.transform(scaled_data)\n",
    "    model_prediction = best_estimator.predict(scaled_data)\n",
    "    if model_prediction[0] == 0 :\n",
    "        return \"Water is NOT SAFE for Consumption\"\n",
    "    else : \n",
    "        return \"Water is SAFE for Consumption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbacd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = float(input('Enter the pH Value = '))\n",
    "Hardness = float(input('Enter the Hardness Value = '))\n",
    "Solids = float(input('Enter the Solids Value = '))\n",
    "Chloramines = float(input('Enter the Chloramines Value  = '))\n",
    "Sulfate = float(input('Enter the Sulfate Value = '))\n",
    "Conductivity = float(input('Enter the Conductivity Value = '))\n",
    "Organic_carbon = float(input('Enter the Organic_Carbon Value = '))\n",
    "Trihalomethanes = float(input('Enter the Trihalomethanes Value = '))\n",
    "Turbidity = float(input('Enter the Turbidity Value = '))\n",
    "\n",
    "input_data = [ph, Hardness, Solids, Chloramines, Sulfate, Conductivity,\n",
    "             Organic_carbon, Trihalomethanes, Turbidity]\n",
    "water_Quality_Prediction(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab755b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c00ba",
   "metadata": {},
   "source": [
    "#### Save Trained Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e13274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e566f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Water_Quality_ML_Trained_Model.sav'\n",
    "pickle.dump(best_estimator, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408b8ee",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423ae4b",
   "metadata": {},
   "source": [
    "#### Load Trained Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_quality_model = pickle.load(open('Water_Quality_ML_Trained_Model.sav', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118add35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [7.56, 237, 14245, 6.289, 373, 10.47, 85.9, 2.44]\n",
    "scaled_data1 = std_scaler.transform([data])\n",
    "water_quality_model.predict(scaled.data1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
